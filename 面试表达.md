## 自我介绍：
面试官你好，我叫陶真凯，19年毕业于淮南师范学院计算机科学与技术专业，目前工作已有近6年，共有两份工作经历。

19年-21年在上海建朗信息科技有限公司担任中级Java开发工程师，主要负责公司核心业务工会业务的开发工作。

之后来到杭州，入职中控信息产业股份有限公司，研发全息数字道路和越安行这两个产品。产品开发的过程中，我主要负责需求评审与架构设计，部门之间的协作沟通和产品落地以及产品上线后的困难问题攻关，推动产品标准化部署工作。

在GAP期间，我也在扩展自己的技术能力，阅读一些架构方面的书籍如DDIA（设计数据密集型应用）、周志明凤凰架构、软件架构之道。

希望我有机会能进入贵司，用我的工作经验与技术为公司创造价值。谢谢

项目说辞：
S: 数字道路是公司基于原有的智慧路口产品，进行战略升级，整合公司自有资源和数字孪生技术来做的一个全息路口数字化解决方案

T：我在这个产品研发中担任核心开发的角色，主要负责跟产品经理确定功能需求、任务分配以及部门之间的协调沟通工作：

- 数据接入这一块，本身是自己开发，自己接入；后面公司在内部推中台产品的落地，所以雷达、卡口还有什么的数据一般都统一交给中台部门来接入了，其他业务负责消费中台接入的数据，做产品的研发工作。
- 桐乡客户有个需求，就是他们的指路牌还有屏幕来显示这条路段的通行时间以及通行状态。这个是我们之前没有遇到过的设备，需求下来的时候，我就快速的拆解这个需求，提出了这个需求的解决方案，大概就是
	- 通行时间还有通行状态这个指标可以由算法部门基于雷达或者卡口的数据来计算
	- 我这边负责对指路牌信息维护，前端建模和人工数据下发功能开发以及算法的结果数据下发
	- 小站那边提供API，来保证我这边算法或者人工数据的下发功能
	- 结果的话就是我们在一周内完成功能从需求到功能上线，数据联调，然后周末临时加一天班对算法结果准确性进行调试，保障客户周一的演示

其中一个困难：
A：比如在产品落地后，我们在滨江交警那边发现我们过车的实时展示跟现实情况对比会有大概5s左右的延迟情况。这个情况经过我们的走查在其他地方是没有出现的。后面我们就分析了一下这个现象出现的情况：
- 订阅路口多，数据量大：对比其他地方，滨江这边属于路口数量很多 大概有3  40个订阅路口这个样子，属于高峰期数据量巨大。
- 定时推送一批数据，避免实时数据对前端页面造成压力：代码层面我们是按照定时任务一批一批推送的，因为如果实时推送的话，前端页面也顶不住实时渲染的压力。所以我们只能从后端方面去对需要推送的数据做精简。
- 单一Topic推送 -> 按路口分配topic：根据订阅的路口信息，动态创建KafkaListenerContiner，通过定时轮询被监听的路口信息，配合心跳机制来控制Listener容器的启停
R：后面通过对高峰期进行路口订阅观察，就没有此延时的情况，不过还有一个小的问题就是切换路口时，有个延迟；所以保留了之前的实时订阅，开放给路口数量少的项目中。

另外一个困难：
视频事件接入链路优化：
S：功能上线之后，在滨江早晚高峰期会出现频繁性崩溃，K8s都给服务重启了1000多次
T：我把日志给下载出来，得到OOM异常，后面调大JVM堆大小，也只是延缓崩溃的时间
A：
- 首先是先分析了一下接收的数据中，数据量较多的事件类型和等级比较低的事件类型，停止接收一些等级比较低且数量较大的事件
- 调整代码，查看单一数据处理时长，发现一个10M左右的视频在局域网下需要大概2 3s左右，这是很不合理的。所以才造成堆内大量视频文件对象。对象存活时间太久，新的对象又源源不断产生了，后面申请不到新的内存，只能崩溃了
- 先将视频文件使用NIO保存本地，再封装文件地址信息和事件ID到kafka msg里面异步去消费，成功后删除，避免占用过多磁盘


个人优势：
- 6年工作经验 java基础扎实，有丰富的项目经验
- 拥有自驱力，能够自主学习新知识：学过go rust 目前也在看架构方面的书 精进自己的内功 也有自己的技术博客
- 责任心强，有需求下来一般会主动推进工作，将需求快速落地，快速迭代
	